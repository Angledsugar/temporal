<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="TempoRAL discovers cross-embodiment temporal abstractions from human demonstrations via internal RL in VLM backbones.">
  <meta name="keywords" content="TempoRAL, VLA, hierarchical RL, temporal abstraction, robot learning, VLM backbone, cross-embodiment">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TempoRAL: Cross-Embodiment Temporal Abstraction Transfer via Internal RL in VLM Backbones</title>

  <!-- Fonts: Google Sans + Noto Sans + Castoro (same as Nerfies) -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <!-- Bulma CSS Framework -->
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">

  <!-- Academicons (for arXiv icon, same as Nerfies) -->
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <!-- KaTeX for math rendering -->
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false}
          ]
        });"></script>

  <!-- Custom CSS -->
  <link rel="stylesheet" href="./static/css/index.css">

  <!-- jQuery (Nerfies pattern) -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

  <!-- Custom JS -->
  <script defer src="./static/js/index.js"></script>
</head>
<body>


<!-- ===== NAVBAR ===== -->
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="#">
        <span class="icon"><i class="fas fa-home"></i></span>
      </a>
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">More Research</a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://www.physicalintelligence.company/blog/pi0">
            &pi;<sub>0</sub>
          </a>
          <a class="navbar-item" href="https://www.physicalintelligence.company/blog/pi05">
            &pi;<sub>0.5</sub>
          </a>
          <a class="navbar-item" href="https://arxiv.org/abs/2512.20605">
            Internal RL (Kobayashi et al.)
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>


<!-- ===== HERO: Title, Authors, Links ===== -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <span class="temporal-highlight">TempoRAL</span>: Cross-Embodiment Temporal Abstraction
            Transfer from Human Demonstrations via Internal Reinforcement
            Learning in VLM Backbones
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Anonymous Authors</span>
          </div>
          <div class="is-size-6 publication-authors" style="margin-top: 4px; color: #7f8c8d;">
            <span class="author-block">Under Review</span>
          </div>

          <!-- Publication Links -->
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>


<!-- ===== TEASER: TL;DR ===== -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        <span class="temporal-highlight">TempoRAL</span> discovers temporal abstractions
        from the VLM backbone's internal representations, enabling
        <strong>cross-embodiment temporal abstraction transfer</strong>
        from human demonstrations to robots.
      </h2>
    </div>
  </div>
</section>


<!-- ===== PIPELINE OVERVIEW ===== -->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Three-Phase Pipeline</h2>
      <div style="overflow-x: auto;">
        <div class="pipeline-flow">
          <div class="pipeline-phase phase-1">
            <div class="phase-label">Phase 1</div>
            <div class="phase-title">VLM Fine-Tune</div>
            <div class="phase-detail">Human demo LoRA<br>on VLM backbone, then freeze</div>
          </div>
          <div class="pipeline-arrow">&rarr;</div>
          <div class="pipeline-phase phase-2">
            <div class="phase-label">Phase 2</div>
            <div class="phase-title">MC Discovery</div>
            <div class="phase-detail">Self-supervised<br>boundary discovery on<br>frozen VLM residual stream</div>
          </div>
          <div class="pipeline-arrow">&rarr;</div>
          <div class="pipeline-phase phase-3">
            <div class="phase-label">Phase 3</div>
            <div class="phase-title">Internal RL</div>
            <div class="phase-detail">Novel task composition<br>in abstract-action space</div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- ===== ABSTRACT + KEY HYPOTHESIS ===== -->
<section class="section">
  <div class="container is-max-desktop">

    <!-- Abstract -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision-Language-Action (VLA) systems such as &pi;<sub>0.5</sub> and GR00T adopt hierarchical architectures
            in which a Vision-Language Model (VLM) backbone decomposes instructions into subtasks executed by an action expert.
            We hypothesise that the VLM backbone &mdash; an autoregressive, causal transformer &mdash; develops
            <em>temporally-abstract representations</em> of manipulation subtasks in its residual stream, and that
            these representations are <strong>embodiment-invariant</strong>: a human grasping a cup and a robot grasping
            a cup share the same reach&rarr;approach&rarr;close&rarr;lift boundary pattern despite vastly different kinematics.
          </p>
          <p>
            Building on this insight, we propose <strong>TempoRAL</strong>, a three-phase framework:
            (1)&nbsp;fine-tune the VLM backbone on human manipulation demonstrations so that manipulation-specific
            temporal structure is encoded in its internal representations, then freeze;
            (2)&nbsp;train a self-supervised meta-controller on the frozen VLM backbone's residual stream to discover
            subtask boundaries without any annotation; and
            (3)&nbsp;apply Internal Reinforcement Learning in the discovered abstract-action space so that a causal
            policy can compose novel subtask sequences for new tasks under sparse rewards.
          </p>
          <p>
            A key ablation compares a base VLM (pretrained only) against a human-data-fine-tuned VLM, isolating the
            contribution of human demonstrations to temporal abstraction formation.
            We argue that this constitutes a novel form of <em>temporal abstraction transfer</em> &mdash; going beyond
            the representation-level cross-embodiment alignment reported in prior work to transfer the
            <em>temporal structure</em> of manipulation from humans to robots.
          </p>
        </div>
      </div>
    </div>

    <!-- Key Hypothesis -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Why VLM Backbone, Not Action Expert?</h2>
        <div class="content has-text-justified">
          <p>
            A critical prerequisite of Internal RL is that the base model must be
            <strong>autoregressive and causal</strong> &mdash; temporal abstractions emerge from
            sequential next-token prediction over long horizons.
          </p>
          <table class="table is-striped is-fullwidth">
            <thead>
              <tr>
                <th>Property</th>
                <th>VLM Backbone</th>
                <th>Action Expert</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>Prediction</strong></td>
                <td>Autoregressive (next-token)</td>
                <td>Diffusion / flow matching</td>
              </tr>
              <tr>
                <td><strong>Attention</strong></td>
                <td>Causal (past &rarr; future)</td>
                <td>Bidirectional (full chunk)</td>
              </tr>
              <tr>
                <td><strong>Temporal scope</strong></td>
                <td>Episode-length sequences</td>
                <td>Short action chunks (0.1&ndash;1s)</td>
              </tr>
              <tr>
                <td><strong>Role</strong></td>
                <td>Planner ("what to do next")</td>
                <td>Executor ("how to move")</td>
              </tr>
              <tr>
                <td><strong>Temporal abstraction</strong></td>
                <td class="has-text-success has-text-weight-bold">Emerges naturally</td>
                <td class="has-text-danger has-text-weight-bold">Cannot form</td>
              </tr>
            </tbody>
          </table>
          <p>
            <strong>Evidence:</strong> GR00T&nbsp;N1 found that <em>middle-layer</em> VLM embeddings outperform
            final-layer embeddings for downstream policy success.
            &pi;<sub>0.5</sub> already predicts subtask text explicitly within the VLM.
          </p>
        </div>
      </div>
    </div>

  </div>
</section>


<!-- ===== METHOD ===== -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method</h2>

        <!-- Phase 1: VLM Fine-Tuning -->
        <div class="box method-card card-phase1">
          <h3 class="title is-4">
            <span class="tag is-danger">Phase 1</span>
            &nbsp;VLM Backbone Fine-Tuning on Human Demonstrations
          </h3>
          <div class="content has-text-justified">
            <p>
              Fine-tune the VLM backbone on human manipulation demonstrations using next-token prediction
              over observation-action sequences, then <strong>freeze</strong> for all subsequent phases.
            </p>

            <!-- Architecture specs -->
            <div class="columns is-mobile is-multiline" style="margin-top: 1rem;">
              <div class="column is-3-desktop is-6-mobile has-text-centered">
                <div class="spec-value">1.7&ndash;2B</div>
                <div class="spec-label">VLM Parameters</div>
              </div>
              <div class="column is-3-desktop is-6-mobile has-text-centered">
                <div class="spec-value">2048</div>
                <div class="spec-label">Hidden Width</div>
              </div>
              <div class="column is-3-desktop is-6-mobile has-text-centered">
                <div class="spec-value">18&ndash;28</div>
                <div class="spec-label">Layers</div>
              </div>
              <div class="column is-3-desktop is-6-mobile has-text-centered">
                <div class="spec-value">LoRA</div>
                <div class="spec-label">Fine-Tuning</div>
              </div>
            </div>

            <div class="math-block">
              $$\mathcal{L}(\theta) = \sum_{t=1}^{T} \left[ -\ln p_\theta(a_t \mid o_{1:t}) - \lambda \ln p_\theta(o_{t+1} \mid o_{1:t}) \right]$$
            </div>

            <p>
              <strong>Why human data:</strong> Human manipulation datasets are orders of magnitude larger than
              robot data (Ego4D: 3,670 hrs vs Open X-Embodiment: ~10<sup>5</sup> trajectories).
              The temporal boundary structure (reach&rarr;grasp&rarr;lift&rarr;place) is
              <strong>embodiment-invariant</strong> &mdash; shared across humans and robots.
            </p>
            <p>
              <strong>After this phase, &theta; is frozen permanently.</strong>
              Co-training with the meta-controller causes temporal abstractions to collapse
              (rate-distortion analysis in Kobayashi et al.).
            </p>
          </div>
        </div>

        <!-- Phase 2: MetaController Discovery -->
        <div class="box method-card card-phase2">
          <h3 class="title is-4">
            <span class="tag is-warning">Phase 2</span>
            &nbsp;Meta-Controller Discovery (Self-Supervised)
          </h3>
          <div class="content has-text-justified">
            <p>
              Extract the residual stream at mid-depth from the frozen VLM backbone
              (e.g. layer 14 of 28 for Eagle, layer 9 of 18 for PaliGemma),
              then train a meta-controller (~2M params) to discover subtask boundaries
              &mdash; purely self-supervised, no boundary annotations.
            </p>
            <ul>
              <li><strong>Encoder</strong> (BiGRU): Processes full sequence &rarr; per-timestep latent statistics (&mu;<sub>t</sub>, &sigma;<sub>t</sub>)</li>
              <li><strong>Switching Unit</strong>: Predicts &beta;<sub>t</sub> &isin; [0,1] &mdash; switch to new controller or maintain</li>
              <li><strong>Decoder</strong> (Hypernetwork): z<sub>t</sub> &rarr; low-rank controller U<sub>t</sub> = B<sub>t</sub>A<sub>t</sub></li>
            </ul>

            <div class="math-block">
              $$\mathcal{L}(\phi) = \sum_{t=1}^{T} \left[ -\ln p_{\theta,\phi}(a_t \mid o_{1:t}, z_{1:t}) + \alpha \, D_{\text{KL}}\!\left(\mathcal{N}(\mu_t, \sigma_t^2) \| \mathcal{N}(0, I)\right) \right]$$
            </div>

            <p>
              The additive residual stream intervention keeps the VLM backbone frozen:
            </p>
            <div class="math-block">
              $$e'_{t,l} = e_{t,l} + U_t \, e_{t,l}, \quad U_t = B_t A_t \in \mathbb{R}^{2048 \times 2048}, \; \text{rank} = 32$$
            </div>
          </div>
        </div>

        <!-- Phase 3: Internal RL -->
        <div class="box method-card card-phase3">
          <h3 class="title is-4">
            <span class="tag is-success">Phase 3</span>
            &nbsp;Internal RL for Novel Task Composition
          </h3>
          <div class="content has-text-justified">
            <p>
              Construct an "internal" MDP where the state is the VLM backbone's residual stream,
              the action is a controller code, and the VLM + action expert + physical world form
              the environment dynamics.
              Only the causal RL policy (&psi;) is trained; everything else is frozen.
            </p>

            <table class="table is-bordered is-fullwidth">
              <thead>
                <tr><th>Component</th><th>Definition</th></tr>
              </thead>
              <tbody>
                <tr><td><strong>State</strong></td><td>VLM residual stream $e_{t,l} \in \mathbb{R}^{2048}$</td></tr>
                <tr><td><strong>Action</strong></td><td>Controller code $z_t \in \mathbb{R}^{16}$</td></tr>
                <tr><td><strong>Reward</strong></td><td>Binary task success (sparse)</td></tr>
              </tbody>
            </table>

            <div class="math-block">
              $$\nabla_\psi J = \mathbb{E}\!\left[\sum_{m=1}^{M} A_m \, \nabla_\psi \ln \pi_\psi(z_{t_m} \mid e_{1:t_m, l})\right]$$
            </div>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>


<!-- ===== EMERGENT BOUNDARIES ===== -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Emergent Subtask Boundaries</h2>
        <div class="content has-text-justified">
          <p>
            After meta-controller training, the switching gate &beta;<sub>t</sub> learns sparse, quasi-binary firing patterns
            that align with semantic manipulation transitions &mdash; <strong>without any boundary supervision</strong>.
            This reflects the VLM backbone's internal notion of "where one coherent behavioural plan ends and another begins."
          </p>
        </div>

        <!-- Beta Visualization -->
        <div class="beta-viz">
          <div class="viz-title">Learned &beta;<sub>t</sub> Pattern (No Boundary Supervision)</div>
          <div class="beta-row">
            <span class="beta-label">Time</span>
            <div class="beta-values">
              <div class="beta-cell" style="background:none; color:#95a5a6;">1</div>
              <div class="beta-cell" style="background:none; color:#95a5a6;">2</div>
              <div class="beta-cell" style="background:none; color:#95a5a6;">3</div>
              <div class="beta-cell" style="background:none; color:#95a5a6;">4</div>
              <div class="beta-cell" style="background:none; color:#95a5a6;">5</div>
              <div class="beta-cell" style="background:none; color:#95a5a6;">6</div>
              <div class="beta-cell" style="background:none; color:#95a5a6;">7</div>
              <div class="beta-cell" style="background:none; color:#95a5a6;">8</div>
              <div class="beta-cell" style="background:none; color:#95a5a6;">9</div>
              <div class="beta-cell" style="background:none; color:#95a5a6;">10</div>
              <div class="beta-cell" style="background:none; color:#95a5a6;">11</div>
              <div class="beta-cell" style="background:none; color:#95a5a6;">12</div>
              <div class="beta-cell" style="background:none; color:#95a5a6;">13</div>
              <div class="beta-cell" style="background:none; color:#95a5a6;">14</div>
              <div class="beta-cell" style="background:none; color:#95a5a6;">15</div>
            </div>
          </div>
          <div class="beta-row">
            <span class="beta-label">&beta;<sub>t</sub></span>
            <div class="beta-values">
              <div class="beta-cell beta-high">1</div>
              <div class="beta-cell beta-low">0</div>
              <div class="beta-cell beta-low">0</div>
              <div class="beta-cell beta-low">0</div>
              <div class="beta-cell beta-high">1</div>
              <div class="beta-cell beta-low">0</div>
              <div class="beta-cell beta-low">0</div>
              <div class="beta-cell beta-high">1</div>
              <div class="beta-cell beta-low">0</div>
              <div class="beta-cell beta-low">0</div>
              <div class="beta-cell beta-low">0</div>
              <div class="beta-cell beta-low">0</div>
              <div class="beta-cell beta-high">1</div>
              <div class="beta-cell beta-low">0</div>
              <div class="beta-cell beta-low">0</div>
            </div>
          </div>
          <div class="beta-row">
            <span class="beta-label"></span>
            <div class="beta-annotations">
              <div class="beta-anno-cell">&uarr;</div>
              <div class="beta-anno-cell"></div>
              <div class="beta-anno-cell"></div>
              <div class="beta-anno-cell"></div>
              <div class="beta-anno-cell">&uarr;</div>
              <div class="beta-anno-cell"></div>
              <div class="beta-anno-cell"></div>
              <div class="beta-anno-cell">&uarr;</div>
              <div class="beta-anno-cell"></div>
              <div class="beta-anno-cell"></div>
              <div class="beta-anno-cell"></div>
              <div class="beta-anno-cell"></div>
              <div class="beta-anno-cell">&uarr;</div>
              <div class="beta-anno-cell"></div>
              <div class="beta-anno-cell"></div>
            </div>
          </div>
          <div class="beta-row">
            <span class="beta-label"></span>
            <div class="beta-annotations">
              <div class="beta-anno-cell" style="width: 126px; justify-content: center; color: #e74c3c; font-size: 0.8em;">reach</div>
              <div class="beta-anno-cell" style="width: 42px;"></div>
              <div class="beta-anno-cell" style="width: 84px; justify-content: center; color: #f39c12; font-size: 0.8em;">grasp</div>
              <div class="beta-anno-cell" style="width: 42px;"></div>
              <div class="beta-anno-cell" style="width: 168px; justify-content: center; color: #27ae60; font-size: 0.8em;">lift</div>
              <div class="beta-anno-cell" style="width: 42px;"></div>
              <div class="beta-anno-cell" style="width: 84px; justify-content: center; color: #3498db; font-size: 0.8em;">place</div>
            </div>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>


<!-- ===== TEMPORAL CONTRACTION + ABLATION ===== -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">

        <!-- Temporal Contraction -->
        <h2 class="title is-3">Temporal Contraction</h2>
        <div class="content has-text-justified">
          <p>
            Internal RL's key advantage: instead of making a decision at every timestep (standard RL),
            the policy acts only at switching points where &beta;<sub>t</sub> fires.
            This dramatically reduces the search space and improves credit assignment.
          </p>
        </div>

        <div class="columns">
          <div class="column">
            <div class="box contraction-box standard has-text-centered">
              <p class="heading">Standard RL</p>
              <p class="title is-2 has-text-danger">T = 50</p>
              <p class="subtitle is-6">decisions per episode<br>(every timestep)</p>
            </div>
          </div>
          <div class="column">
            <div class="box contraction-box temporal has-text-centered">
              <p class="heading">Internal RL</p>
              <p class="title is-2 has-text-success">M = 4</p>
              <p class="subtitle is-6">decisions per episode<br>(switching points only)</p>
            </div>
          </div>
        </div>

        <p class="has-text-centered is-size-5 has-text-weight-semibold has-text-success" style="margin-bottom: 2rem;">
          12.5&times; reduction in search space &rarr; dramatically lower gradient variance
        </p>

        <hr>

        <!-- Proposed Ablation -->
        <h2 class="title is-3">Proposed Ablation: Human Data Contribution</h2>
        <div class="content has-text-justified">
          <p>
            The central experiment isolates whether human demonstration data enhances temporal
            abstraction formation in the VLM backbone:
          </p>
        </div>

        <table class="table is-striped is-fullwidth">
          <thead>
            <tr>
              <th>Condition</th>
              <th>VLM State</th>
              <th>Fine-Tune Data</th>
              <th>Expected Outcome</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>A (Base)</strong></td>
              <td>Pretrained only</td>
              <td>None</td>
              <td>Weak temporal abstractions</td>
            </tr>
            <tr>
              <td><strong>B (Human)</strong></td>
              <td>+ LoRA fine-tune</td>
              <td>Human demonstrations</td>
              <td>Strong temporal abstractions</td>
            </tr>
            <tr>
              <td><strong>C (Robot)</strong></td>
              <td>+ LoRA fine-tune</td>
              <td>Robot demonstrations</td>
              <td>Comparable to B (cross-embodiment transfer)</td>
            </tr>
          </tbody>
        </table>

        <div class="content has-text-justified">
          <p>
            <strong>Metrics:</strong>
            (1) Linear probing accuracy for subtask decoding from VLM residual stream,
            (2) &beta;<sub>t</sub> alignment with ground-truth subtask boundaries (NMI),
            (3) Internal RL success rate on novel task compositions.
          </p>
          <p>
            If <strong>A &lt; B</strong>: human fine-tuning enhances temporal abstractions.
            If <strong>B &asymp; C</strong>: strong evidence for <em>cross-embodiment temporal abstraction transfer</em>.
            If <strong>B &gt; C</strong>: human data produces <em>superior</em> temporal abstractions.
          </p>
        </div>

      </div>
    </div>
  </div>
</section>


<!-- ===== TARGET VLA ARCHITECTURES ===== -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Target VLA Architectures</h2>
        <div class="content has-text-justified">
          <table class="table is-bordered is-striped is-fullwidth">
            <thead>
              <tr>
                <th>Model</th>
                <th>VLM Backbone</th>
                <th>Layers</th>
                <th>Hidden Dim</th>
                <th>Controlled Layer</th>
                <th>Action Expert</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>&pi;<sub>0.5</sub></td>
                <td>PaliGemma (Gemma 2B)</td>
                <td>18</td>
                <td>2048</td>
                <td>9 (L/2)</td>
                <td>Gemma Expert 300M</td>
              </tr>
              <tr>
                <td>GR00T N1.6</td>
                <td>Eagle-2 (Qwen3 1.7B)</td>
                <td>28</td>
                <td>2048</td>
                <td>14 (L/2)</td>
                <td>DiT 32L</td>
              </tr>
              <tr>
                <td>Qwen2.5-VL-7B</td>
                <td>Qwen2.5</td>
                <td>28</td>
                <td>3584</td>
                <td>14 (L/2)</td>
                <td>(to be added)</td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- ===== KEY DESIGN INSIGHTS ===== -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Key Design Insights</h2>
        <div class="content has-text-justified">
          <ol>
            <li>
              <strong>Frozen VLM backbone is critical.</strong>
              Co-training the VLM with the meta-controller causes temporal abstractions to collapse.
              Freezing &theta; creates an information bottleneck that forces &beta;<sub>t</sub> to capture
              genuine temporal structure.
            </li>
            <li>
              <strong>Embodiment-invariant temporal boundaries.</strong>
              The temporal boundary pattern (reach &rarr; grasp &rarr; lift &rarr; place) is shared across embodiments,
              enabling pretraining on virtually unlimited human data
              (~10<sup>8</sup> samples vs ~10<sup>5</sup> robot trajectories).
            </li>
            <li>
              <strong>VLM backbone, not action expert.</strong>
              The VLM is autoregressive and causal &mdash; exactly the setting where temporal abstractions
              emerge. The action expert (diffusion/flow matching, bidirectional) cannot form them.
            </li>
            <li>
              <strong>Residual stream intervention, not weight modification.</strong>
              The meta-controller applies additive control on VLM residual streams:
              e' = e + U&middot;e. This adds only ~2M parameters on top of the frozen 1.7&ndash;2B VLM backbone.
            </li>
            <li>
              <strong>Non-causal to causal transition.</strong>
              Phase 2 uses a non-causal BiGRU (sees future) for optimal boundary discovery.
              Phase 3 replaces this with a causal GRU for online execution.
            </li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- ===== RELATED WORK ===== -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Work</h2>
        <div class="content has-text-justified">

          <h3 class="title is-4">Vision-Language-Action Models</h3>
          <p>
            RT-2, &pi;<sub>0</sub>, &pi;<sub>0.5</sub>, OpenVLA, Octo, and GR00T N1 have advanced VLA capabilities,
            but all treat the subtask&ndash;action interface as a fixed design choice.
            None exploits the VLM backbone's internal temporal representations to discover subtask boundaries from data.
          </p>

          <h3 class="title is-4">Temporal Abstraction &amp; Hierarchical RL</h3>
          <p>
            The Options framework, Option-Critic, HIRO, and CompILE address temporal abstraction in RL.
            Most recently, Kobayashi et al. (2025) proposed Internal RL, showing that a meta-controller
            trained on a frozen autoregressive model's residual stream discovers temporal abstractions
            aligned with ground-truth subgoals. Internal RL has only been demonstrated in gridworld and
            MuJoCo; its potential for VLM backbones in VLA systems is unexplored.
          </p>

          <h3 class="title is-4">Learning from Human Data</h3>
          <p>
            LAPA, MT-&pi;, and Being-H0 transfer low-level actions or visual features from human data.
            Physical Intelligence's analysis of &pi;<sub>0.5</sub> revealed emergent cross-embodiment alignment
            at scale. We go further: transferring <em>temporal abstraction structure</em>, which is more abstract
            and therefore more embodiment-invariant than representations alone.
          </p>

        </div>
      </div>
    </div>
  </div>
</section>


<!-- ===== BIBTEX ===== -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{temporal2025,
  title     = {TempoRAL: Cross-Embodiment Temporal Abstraction Transfer
               from Human Demonstrations via Internal Reinforcement
               Learning in VLM Backbones},
  author    = {Anonymous},
  journal   = {Under Review},
  year      = {2025}
}</code></pre>
  </div>
</section>


<!-- ===== REFERENCES ===== -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">References</h2>
        <div class="content references">
          <ol>
            <li>Physical Intelligence. <em>&pi;<sub>0</sub>: A Vision-Language-Action Flow Model for General Robot Control.</em> arXiv:2410.24164, 2024.</li>
            <li>Physical Intelligence. <em>&pi;<sub>0.5</sub>: A Vision-Language-Action Model with Open-World Generalization.</em> 2025.</li>
            <li>Physical Intelligence. <em>&pi;<sub>0</sub>-FAST: Efficient Action Tokenization for Vision-Language-Action Models.</em> 2025.</li>
            <li>Physical Intelligence. <em>Emergent Cross-Embodiment Alignment in Scaled VLA Models.</em> Blog post, 2025.</li>
            <li>S. Kobayashi et al. <em>Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning.</em> arXiv:2512.20605, 2025.</li>
            <li>A. Brohan et al. <em>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control.</em> arXiv:2307.15818, 2023.</li>
            <li>M. Kim et al. <em>OpenVLA: An Open-Source Vision-Language-Action Model.</em> arXiv:2406.09246, 2024.</li>
            <li>Octo Model Team. <em>Octo: An Open-Source Generalist Robot Policy.</em> arXiv:2405.12213, 2024.</li>
            <li>Open X-Embodiment Collaboration. <em>Open X-Embodiment: Robotic Learning Datasets and RT-X Models.</em> arXiv:2310.08864, 2023.</li>
            <li>K. Grauman et al. <em>Ego4D: Around the World in 3,000 Hours of Egocentric Video.</em> CVPR, 2022.</li>
            <li>NVIDIA. <em>GR00T N1: An Open Foundation Model for Generalist Humanoid Robots.</em> arXiv:2503.14734, 2025.</li>
            <li>E. J. Hu et al. <em>LoRA: Low-Rank Adaptation of Large Language Models.</em> ICLR, 2022.</li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- ===== FOOTER ===== -->
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
